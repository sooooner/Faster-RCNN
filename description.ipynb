{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latest-singles",
   "metadata": {},
   "source": [
    "Kaggle에서 제공하는 Crowdflower Search Results Relevance data를 사용하여 검색 서비스 만족도 판별 모델을 개발한 과정을 정리해 봤습니다.\n",
    "\n",
    "\n",
    "dacon에서 제공하는 데이터를 사용하여 객체 인식 알고리즘인 faster rcnn을 개발한 과정을 정리해 봤습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-cookbook",
   "metadata": {},
   "source": [
    "𝑒32⋅2𝑒2𝜋𝑖 𝑑𝑑𝑥∫𝑏𝑎∑𝑖=1𝑁𝑓(𝑥)𝑑𝑥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-concern",
   "metadata": {},
   "source": [
    "$$e^\\frac{3}{2} \\cdot 2 e^{2 \\pi i } \\  \\frac{d}{dx}\\int_{a}^{b}\\sum_{i=1}^{N}f(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-aquatic",
   "metadata": {},
   "source": [
    "먼저 human joint keypoint detection task의 동향에 대하여 알아보기 위하여 역대 SOTA를 달성한 논문들을 읽어 보았습니다.\n",
    "결과로 여러가지 방식중 사람을 먼저 detection한뒤 keypoint를 추정하는 Top-down방식이 \n",
    "\n",
    "하지만 안그래도 늦게 시작하였는데 논문을 읽으며 개발 과정을 탐구 하는데 시간을 너무 많이 허비 하여,\n",
    "detection model만이라도 개발해보자하여 faster-rcnn을 적용하고 시간이 남는다면 mask rcnn을 적용하여 key-point를 추정하기로 계획을 변경하였습니다.\n",
    "\n",
    "detection model을 개발해보는 것만으로도 좋은 경험이므로 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-christian",
   "metadata": {},
   "source": [
    "아래는 faster rcnn 구현 코드와 설명입니다.\n",
    "모든 코드는 google colab에서 구동하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-tutorial",
   "metadata": {},
   "source": [
    "backbone은 resnet50을 사용하였습니다.\n",
    "또한 이미지가 너무 크기 때문에 0.4배 하여 사용하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-warrior",
   "metadata": {},
   "source": [
    "먼저 dacon에서 제공하는 data는 key point 추정이므로 bounding box에 대한 라벨은없습니다.\n",
    "따라서 각 key point에서 x, y축 최대, 최소 값을 기준으로x, y축 최대, 최소 값을 기준으로 bounding box를 설정하기로 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_generator(target):\n",
    "    x_min = np.min(target[::2])\n",
    "    x_max = np.max(target[::2])\n",
    "    y_min = np.min(target[1::2])\n",
    "    y_max = np.max(target[1::2])\n",
    "    ratio = 'w' if x_max - x_min > y_max - y_min else 'h'\n",
    "\n",
    "    x_min = x_min - (x_max - x_min)*.1 if ratio == 'h' else x_min\n",
    "    x_max = x_max + (x_max - x_min)*.1 if ratio == 'h' else x_max\n",
    "    y_min = y_min - (y_max - y_min)*.1 if ratio == 'w' else y_min\n",
    "    y_max = y_max + (y_max - y_min)*.1 if ratio == 'w' else y_max\n",
    "\n",
    "    ground_truth_x_min = x_min - (x_max - x_min)*.05\n",
    "    ground_truth_x_max = x_max + (x_max - x_min)*.05\n",
    "    ground_truth_y_min = y_min - (y_max - y_min)*.05\n",
    "    ground_truth_y_max = y_max + (y_max - y_min)*.05\n",
    "\n",
    "    ground_truth_w = ground_truth_x_max - ground_truth_x_min\n",
    "    ground_truth_h = ground_truth_y_max - ground_truth_y_min\n",
    "    ground_truth_x = ground_truth_w/2 + ground_truth_x_min\n",
    "    ground_truth_y = ground_truth_h/2 + ground_truth_y_min\n",
    "    return [ground_truth_x, ground_truth_y, ground_truth_w, ground_truth_h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-mountain",
   "metadata": {},
   "source": [
    "key point에서 y 최소값은 눈, 팔을 들고 있다면 손목, 최댓값은 발목, 등\n",
    "x 최솟값은 손목, 누워있는경우 눈, 발목 등으로 \n",
    "단순히 최대 최소 값을 기준으로 bounding box를 설정하면 잘리기 때문에 가로 세로를 확장하기로 하였습니다.\n",
    "또한 w, h의 비율을 생각하여 w가 길면 누워있는 경우, h가 길면 서 있는 경우임을 확인 하였습니다. \n",
    "비율이 비슷한 경우는 상체를 숙이고 있거나 벤치에 누워서 팔을 들어올린 경우가 있지만 위와 같이 처리하였을때 사람을 잘 포함하기 때문에 이 경우는 따로 처리 하지 않았습니다.\n",
    "\n",
    "즉, 서있는경우와 누워있는 경우의 가로 세로 확장을 다르게 두었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-injection",
   "metadata": {},
   "source": [
    "bounding box를 설정하였다면 anchor box를 설정하여야 합니다. \n",
    "먼저 아래의 기준을 새우고 이를 그래프로 그려 확인하고 결과를 시각적으로 확인하는 과정을 거쳐\n",
    "\n",
    "먼저 아래의 기준으로 ground truth bounding box의 분포를 그래프 눈으로 확인하여 anchor box를 어떻게 설정할지 보아야 합니다.\n",
    "\n",
    "1. ground truth bounding box의 넓이 분포\n",
    "2. w, h의 길이 비\n",
    "3. anchor의 갯수\n",
    "\n",
    "넓이는 140, 160, 180, 210, 240\n",
    "w, h의 비율은 동일한 경우, 2배, 3배로 구분하여 설정하였습니다.\n",
    "따라서 anchor의 갯수는 bin 하나당 5 x 5 개가 생성 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medical-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_box_generator(x, y, scales, ratio):\n",
    "    anchor_boxes = []\n",
    "    for scale in scales:\n",
    "        for w, h  in ratio:\n",
    "            w *= scale\n",
    "            h *= scale     \n",
    "            anchor_boxes.append([x, y, w, h])\n",
    "    return anchor_boxes\n",
    "\n",
    "def Anchor_Boxes(img_shape, scales, ratio, model='vgg'):\n",
    "    '''\n",
    "    input\n",
    "    img_shape : image shape\n",
    "    output \n",
    "    numpy array shape (w * h * k, 4)\n",
    "    '''\n",
    "    if model == 'vgg':\n",
    "        Ratio = 2**4\n",
    "        \n",
    "    w=img_shape[1]//Ratio\n",
    "    h=img_shape[0]//Ratio\n",
    "    \n",
    "    anchor_boxes = []\n",
    "    for x in range(img_shape[1]//w//2, img_shape[1], img_shape[1]//w):\n",
    "        for y in range(img_shape[0]//h//2, img_shape[0], img_shape[0]//h):\n",
    "            anchor_boxes.append(anchor_box_generator(x, y, scales, ratio))\n",
    "    return np.array(anchor_boxes).reshape(-1, 4)\n",
    "\n",
    "scales = [140, 160, 180, 210, 240]\n",
    "ratio = [(1/np.sqrt(3), np.sqrt(3)), \n",
    "         (1/np.sqrt(2), np.sqrt(2)), \n",
    "         (1, 1), \n",
    "         (np.sqrt(2), 1/np.sqrt(2)), \n",
    "         (np.sqrt(3), 1/np.sqrt(3))]\n",
    "anchor_boxes = Anchor_Boxes((432, 768, 3), scales, ratio, model='vgg')\n",
    "\n",
    "bboxes = anchors_to_coordinates(anchor_boxes)\n",
    "out_boundaries_indxes = (np.where(bboxes[:, 0] < 0) or np.where(bboxes[:, 2] < 0) or \n",
    "                         np.where(bboxes[:, 1] > 768) or np.where(bboxes[:, 3] > 432))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import anchors_to_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-terrace",
   "metadata": {},
   "source": [
    "anchor box를 보면 이미지의 범위를 넘는 anchor가 상당수 존재합니다. 학습과정에서 이를 무시 하기 위해서 out_boundaries_indxes를 따로 계산하여 label을 생성할때 무시하도록 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-republic",
   "metadata": {},
   "source": [
    "anchor box를 설정하였으니 ground truth와 비교하여 label을 생성합니다. \n",
    "iou가 각 anchor box들과 ground truth의 iou가 0.7이상인 box를 P, 0.3 이하인 box를 N으로 두고 나머지는 고려 하지 않습니다.\n",
    "\n",
    "이때 P와 N의 비율이 1:1이 되도록 설정하기 위하여 이미지당 label 갯수를 설정 32로 설정 하였습니다. \n",
    "검출하여야 하는 사람이 이미지당 단 하나이며, 자세가 다양하지 않기 때문에 갯수를 32로 설정 하였습니다.\n",
    "\n",
    "추가로 bounding box regression을 수행하기 위하여 좌표를 논문에서 소개된 함수를 통과 시켜주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generator(GT, anchor_boxes, out_boundaries_indxes):\n",
    "    cls_label = - np.ones(shape=(anchor_boxes.shape[0]))\n",
    "    pos_iou_threshold = 0.7\n",
    "    neg_iou_threshold = 0.3\n",
    "    n_sample = 32\n",
    "    pos_ratio = 0.5\n",
    "    n_pos = int(pos_ratio * n_sample)\n",
    "    \n",
    "    ious = np.apply_along_axis(IoU_np, 0, GT, anchor_boxes=anchor_boxes)\n",
    "    cls_label[ious >= pos_iou_threshold] = 1\n",
    "    cls_label[ious < neg_iou_threshold] = 0\n",
    "    cls_label[np.argmax(ious)] = 1\n",
    "    cls_label[out_boundaries_indxes] = -1\n",
    "\n",
    "    pos_index = np.where(cls_label == 1)[0]\n",
    "    if len(pos_index) > n_pos:\n",
    "        disable_index = np.random.choice(\n",
    "            pos_index,\n",
    "            size = (len(pos_index) - n_pos),\n",
    "            replace=False\n",
    "        )\n",
    "        cls_label[disable_index] = -1\n",
    "\n",
    "    n_neg = n_sample - np.sum(cls_label == 1)\n",
    "    neg_index = np.where(cls_label == 0)[0]\n",
    "    if len(neg_index) > n_neg:\n",
    "        disable_index = np.random.choice(\n",
    "            neg_index, \n",
    "            size = (len(neg_index) - n_neg),             \n",
    "            replace = False\n",
    "        )\n",
    "        cls_label[disable_index] = -1\n",
    "\n",
    "    reg_label = anchor_boxes * np.broadcast_to(tf.cast(cls_label > 0, tf.int32), (4, len(cls_label))).T\n",
    "    indices = np.where(reg_label != 0)[0][::4]\n",
    "    x, y, w, h = GT[0], GT[1], GT[2], GT[3]\n",
    "\n",
    "    tx = (x - reg_label[indices][:, 0]) / (reg_label[indices][:, 2])\n",
    "    ty = (y - reg_label[indices][:, 1]) / (reg_label[indices][:, 3])\n",
    "    tw = np.log(w / reg_label[indices][:, 2]) \n",
    "    th = np.log(h / reg_label[indices][:, 3]) \n",
    "    reg_label[indices] = np.stack([tx, ty, tw, th]).T\n",
    "\n",
    "    return cls_label, reg_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-august",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "presidential-burst",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "corrected-asthma",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "manual-bobby",
   "metadata": {},
   "source": [
    "다음은 date augmentation을 수행하는 코드 입니다. 이는 dacon에 코드를 공유하여 주신 분이 계셔서 조금 수정하여 사용 하였습니다.  \n",
    "아래 링크에서 확인 해주세요   \n",
    "[date augmentation](https://dacon.io/competitions/official/235701/codeshare/2383?page=2&dtype=recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traingtGenerator():\n",
    "    Rx, Ry = 0.4, 0.4\n",
    "    image_size = (1080, 1920, 3)\n",
    "    size = [int(image_size[0] * Rx), int(image_size[1] * Ry)]\n",
    "    iter_num = len(train)\n",
    "\n",
    "    for i in range(iter_num):\n",
    "        img = tf.io.read_file(train_val_dir + 'train/' + train['image'].iloc[i]) \n",
    "        img = tf.image.decode_jpeg(img, channels=3) \n",
    "        img = tf.image.resize(img, size) \n",
    "        img = img/255                         \n",
    "        target = list(train.iloc[:,1:49].iloc[i,:])\n",
    "        gt = gt_generator(target)\n",
    "        cls_label, reg_label = label_generator(gt, anchor_boxes, out_boundaries_indxes)\n",
    "\n",
    "        yield img, (cls_label, reg_label, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    traingtGenerator,\n",
    "    output_signature = (\n",
    "            tf.TensorSpec(shape=(size[0], size[1], 3)),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(len(anchor_boxes))),\n",
    "                tf.TensorSpec(shape=(len(anchor_boxes),4)),\n",
    "                tf.TensorSpec(shape=(4))\n",
    "            )\n",
    "        )\n",
    ").batch(batch_size).prefetch(batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-graduation",
   "metadata": {},
   "source": [
    "데이터 로드는 data 함수를 통하여 모델의 input으로 들어가게 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "embedded-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(img_size, model='resnet50'):\n",
    "    if model=='vgg':\n",
    "        base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=img_size)\n",
    "        feature_extractor = base_model.get_layer(\"block5_conv3\")\n",
    "    elif model == 'resnet101':\n",
    "        base_model = tf.keras.applications.ResNet101(include_top=False, weights='imagenet', input_shape=img_size)\n",
    "        feature_extractor = base_model.get_layer(\"conv4_block23_out\")\n",
    "    elif model == 'resnet50':\n",
    "        base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=img_size)\n",
    "        feature_extractor = base_model.get_layer(\"conv4_block6_out\")  \n",
    "    else:\n",
    "        raise Exception('vgg, resnet')\n",
    "\n",
    "    base_model = tf.keras.models.Model(inputs=base_model.input, outputs=feature_extractor.output)\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "useful-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(tf.keras.models.Model):\n",
    "    def __init__(self, img_size, anchor_boxes, k=5*5, n_sample=32, backbone='resnet50', rpn_lambda=10**3, **kwargs):\n",
    "        super(RPN, self).__init__(**kwargs)\n",
    "        self.img_size = img_size\n",
    "        self.anchor_boxes = anchor_boxes\n",
    "        self.num_of_anchor = len(self.anchor_boxes)\n",
    "        self.n_sample = n_sample\n",
    "        self.k = k\n",
    "        self.backbone = backbone\n",
    "        self.rpn_lambda = rpn_lambda\n",
    "\n",
    "        self.base_model = get_base(self.img_size, model=self.backbone)\n",
    "        self.window = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same', name='window')\n",
    "        self.window_bn = tf.keras.layers.BatchNormalization(name='window_bn')\n",
    "        self.window_relu = tf.keras.layers.ReLU(name='window_relu')\n",
    "\n",
    "        self.bbox_reg = tf.keras.layers.Conv2D(filters=self.k*4, kernel_size=1, activation='relu', name='bbox_reg')\n",
    "        self.bbox_reg_reshape = tf.keras.layers.Reshape((-1, 4), name='reg_out')\n",
    "        \n",
    "        self.cls = tf.keras.layers.Conv2D(filters=self.k, kernel_size=1, activation='sigmoid', name='cls')\n",
    "        self.cls_reshape = tf.keras.layers.Reshape((-1, 1), name='cls_out')\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.test_loss_tracker = tf.keras.metrics.Mean(name='test_loss')\n",
    "        self.optimizer = optimizer\n",
    "        super(RPN, self).compile(**kwargs)\n",
    "    \n",
    "    def Cls_Loss(self, y_true, y_pred):\n",
    "        indices = tf.where(tf.not_equal(y_true, tf.constant(-1.0, dtype=tf.float32)))\n",
    "        target = tf.gather_nd(y_true, indices)\n",
    "        output = tf.gather_nd(y_pred, indices)\n",
    "        return tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)(target, output)/self.n_sample\n",
    "\n",
    "    def Reg_Loss(self, y_true, y_pred):\n",
    "        indices = tf.reduce_any(tf.not_equal(y_true, 0), axis=-1)\n",
    "        return tf.losses.Huber(reduction=tf.losses.Reduction.SUM)(y_true[indices], y_pred[indices])/self.num_of_anchor\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            cls, bbox_reg, _ = self(x, training=True)\n",
    "            cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "            reg_loss = self.Reg_Loss(y_reg, bbox_reg)\n",
    "            losses = cls_loss + self.rpn_lambda * reg_loss\n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        grad = tape.gradient(losses, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grad, trainable_vars))\n",
    "        self.loss_tracker.update_state(losses)\n",
    "        return {'rpn_loss': self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        \n",
    "        cls, bbox_reg, _ = self(x, training=False)\n",
    "        cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "        reg_loss = self.Reg_Loss(y_reg, bbox_reg)\n",
    "        losses = cls_loss + self.rpn_lambda * reg_loss\n",
    "\n",
    "        self.test_loss_tracker.update_state(losses)\n",
    "        return {'rpn_loss_val': self.test_loss_tracker.result()}\n",
    "\n",
    "    @tf.function\n",
    "    def bbox_regression(self, boxes):\n",
    "        tx = (boxes[:, :, 0] - self.anchor_boxes[:, 0]) / self.anchor_boxes[:, 2]\n",
    "        ty = (boxes[:, :, 1] - self.anchor_boxes[:, 1]) / self.anchor_boxes[:, 3]\n",
    "        tw = tf.math.log(tf.maximum(boxes[:, :, 2], np.finfo(np.float64).eps) / self.anchor_boxes[:, 2])\n",
    "        th = tf.math.log(tf.maximum(boxes[:, :, 3], np.finfo(np.float64).eps) / self.anchor_boxes[:, 3])\n",
    "        return tf.stack([tx, ty, tw, th], -1)\n",
    "\n",
    "    @tf.function\n",
    "    def inverse_bbox_regression(self, boxes):\n",
    "        gx = self.anchor_boxes[:, 2] * boxes[:, :, 0] + self.anchor_boxes[:, 0]\n",
    "        gy = self.anchor_boxes[:, 3] * boxes[:, :, 1] + self.anchor_boxes[:, 1]\n",
    "        gw = self.anchor_boxes[:, 2] * tf.exp(boxes[:, :, 2])\n",
    "        gh = self.anchor_boxes[:, 3] * tf.exp(boxes[:, :, 3])\n",
    "        return tf.stack([gx, gy, gw, gh], axis=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        feature_extractor = self.base_model(inputs)\n",
    "        intermediate = self.window(feature_extractor)\n",
    "        intermediate = self.window_bn(intermediate)\n",
    "        intermediate = self.window_relu(intermediate)\n",
    "\n",
    "        cls = self.cls(intermediate)\n",
    "        cls = self.cls_reshape(cls)\n",
    "        bbox_reg = self.bbox_reg(intermediate)\n",
    "        bbox_reg = self.bbox_reg_reshape(bbox_reg)\n",
    "        bbox_reg = self.bbox_regression(bbox_reg)\n",
    "        return cls, bbox_reg, feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-johnson",
   "metadata": {},
   "source": [
    "faster rcnn의 region proposal network 구현입니다.\n",
    "get_base로 imagenet에서 pre-train된 resnet50을 불러오고 \n",
    "각 anchor box에서 object가 존재하는지 판단하는 cls branch와 object가 존재한다면 bounbing박스를 예측하는 bbox_reg branch를 이루어져 있습니다. \n",
    "\n",
    "loss는 논문에서 소개한 함수를 사용하였으며 \n",
    "cls의 경우 위에서 -1로 설정한 anchor는 무시하고 계산합니다.\n",
    "bbox reg의 경우 label을 만들때 iou가 0.7 이상인 값에서만 계산 합니다.\n",
    "\n",
    "또한 rpn_lambda는 이미지당 label의 갯수인 n_sample과 anchor box의 갯수인 num_of_anchor의 비율인 10\\*\\*3으로 설정 하였습니다.\n",
    "\n",
    "box regression에서 relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_candidate_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(get_candidate_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def anchors_clip(self, boxes, size=(432, 768)):    \n",
    "        x1 = boxes[:, :, 0] - boxes[:, :, 2]/2\n",
    "        x2 = boxes[:, :, 0] + boxes[:, :, 2]/2\n",
    "        y1 = boxes[:, :, 1] - boxes[:, :, 3]/2\n",
    "        y2 = boxes[:, :, 1] + boxes[:, :, 3]/2\n",
    "        \n",
    "        x1 = tf.clip_by_value(x1, 0, size[1])\n",
    "        x2 = tf.clip_by_value(x2, 0, size[1])\n",
    "        y1 = tf.clip_by_value(y1, 0, size[0])\n",
    "        y2 = tf.clip_by_value(y2, 0, size[0])\n",
    "\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        x = x1 + w/2\n",
    "        y = y1 + h/2\n",
    "        return tf.stack([x, y, w, h], axis=-1)\n",
    "\n",
    "    def call(self, x):\n",
    "        scores, rps, n_pre_nms = x\n",
    "        rois = self.anchors_clip(rps)\n",
    "\n",
    "        oobw = tf.expand_dims(tf.cast(tf.math.greater(rois[:, :, 2], 16), tf.float32), -1)\n",
    "        oobh = tf.expand_dims(tf.cast(tf.math.greater(rois[:, :, 3], 16), tf.float32), -1)\n",
    "        scores = tf.math.multiply(scores, oobw)\n",
    "        scores = tf.math.multiply(scores, oobh)\n",
    "\n",
    "        orders = tf.argsort(scores, direction='DESCENDING', axis=1)[:, :n_pre_nms]\n",
    "        rois = tf.gather_nd(rois, orders, batch_dims=1)\n",
    "        scores = tf.gather_nd(scores, orders, batch_dims=1)\n",
    "        return rois, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-month",
   "metadata": {},
   "source": [
    "rpn을 통과하여 나온 box들을 모두 사용하지 않고 get_candidate_layer통해 아래의 과정을 거칩니다.\n",
    "1. x, y 좌표가 이미지의 사이즈를 넘어가는 경우 clip\n",
    "2. 가로, 세로의 길이가 16 이하인 box들 제외\n",
    "3. confidence score가 높은 순으로 정렬후 상위 n_pre_nms개의 box들만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMS(tf.keras.layers.Layer):\n",
    "    def __init__(self, iou_threshold=0.7, **kwargs):\n",
    "        self.iou_threshold = iou_threshold\n",
    "        super(NMS, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        rois, scores, max_output_size = inputs\n",
    "        selected_indices_padded = tf.image.non_max_suppression_padded(\n",
    "            rois, \n",
    "            tf.squeeze(scores), \n",
    "            max_output_size=max_output_size,\n",
    "            iou_threshold=0.7,\n",
    "            pad_to_max_output_size=True\n",
    "        )[0]\n",
    "        nms = tf.gather(rois, selected_indices_padded, batch_dims=1)\n",
    "        return nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-migration",
   "metadata": {},
   "source": [
    "get_candidate_layer를 통과하여 나온 후보 지역들을 non maximum suppression을 통하여 다시 max_output_size개만 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIpool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=7, **kwargs):\n",
    "        self.pool_size = pool_size\n",
    "        super(RoIpool, self).__init__(**kwargs)\n",
    "\n",
    "    def cal_rois_ratio(self, nmses, size=[432, 768]):\n",
    "        x1 = (nmses[:, :, 0] - nmses[:, :, 2]/2)/size[1]\n",
    "        x2 = (nmses[:, :, 0] + nmses[:, :, 2]/2)/size[1]\n",
    "        y1 = (nmses[:, :, 1] - nmses[:, :, 3]/2)/size[0]\n",
    "        y2 = (nmses[:, :, 1] + nmses[:, :, 3]/2)/size[0]\n",
    "        return tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feature_map, nmses = inputs\n",
    "        n_channel = feature_map.shape[-1]\n",
    "        batch_size = nmses.shape[0]\n",
    "        num_rois = nmses.shape[1]\n",
    "        nmses = self.cal_rois_ratio(nmses)\n",
    "        rois = tf.image.crop_and_resize(\n",
    "            feature_map, \n",
    "            tf.reshape(nmses, (-1, 4)), \n",
    "            box_indices=tf.convert_to_tensor([i for i in range(batch_size) for _ in range(num_rois)]), \n",
    "            crop_size=[self.pool_size, self.pool_size]\n",
    "        )\n",
    "        return tf.reshape(rois, shape=(batch_size, num_rois, self.pool_size, self.pool_size, n_channel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-arbitration",
   "metadata": {},
   "source": [
    "NMS를 통해 나온 지역들은 classifier의 inputs으로 입력되기 위해 roi pooling을 합니다. \n",
    "mask rcnn구현을 찾아보던중 tf.image.crop_and_resize함수가 roi align의 수행과 같은 process라고 하여 간단히 구현 할 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-telephone",
   "metadata": {},
   "source": [
    "classifier를 구현하기 전에 다시 한번 label generator를 구현하여야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(rois, gts):\n",
    "    box1_area = tf.cast(rois[:, :, 2] * rois[:, :, 3], tf.float64)\n",
    "    box2_area = tf.cast(gts[:, 2] * gts[:, 3], tf.float64)\n",
    "    \n",
    "    x1 = tf.maximum(tf.cast(rois[:, :, 0] - rois[:, :, 2]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 0] - gts[:, 2]/2, -1), tf.float64))\n",
    "    x2 = tf.minimum(tf.cast(rois[:, :, 0] + rois[:, :, 2]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 0] + gts[:, 2]/2, -1), tf.float64))\n",
    "    y1 = tf.maximum(tf.cast(rois[:, :, 1] - rois[:, :, 3]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 1] - gts[:, 3]/2, -1), tf.float64))\n",
    "    y2 = tf.minimum(tf.cast(rois[:, :, 1] + rois[:, :, 3]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 1] + gts[:, 3]/2, -1), tf.float64))\n",
    "    \n",
    "    h = tf.maximum(tf.constant(0.0, dtype=tf.float64), y2 - y1)\n",
    "    w = tf.maximum(tf.constant(0.0, dtype=tf.float64), x2 - x1)\n",
    "    \n",
    "    intersect = tf.math.multiply(h, w)\n",
    "    union = tf.subtract(tf.add(box1_area, tf.expand_dims(box2_area, -1)), intersect) + 1e-12\n",
    "    return tf.divide(intersect, union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_label_generator(nms, gts, valid=False):\n",
    "    num_roi = 128\n",
    "    pos_ratio = .5\n",
    "    num_pos = int(num_roi * pos_ratio)\n",
    "    num_neg = int(num_roi - num_pos)\n",
    "    ious = get_iou(nms, gts)\n",
    "    pos_order = tf.argsort(ious * tf.cast(tf.math.greater_equal(ious, 0.5), tf.float64), direction='DESCENDING', axis=1)[:, :num_pos]\n",
    "    neg_order = tf.argsort(ious * tf.cast(tf.math.less(ious, 0.5), tf.float64), direction='DESCENDING', axis=1)[:, :num_neg]\n",
    "    \n",
    "    if not valid:\n",
    "        neg_cnt = tf.reduce_min(tf.math.count_nonzero(ious * tf.cast(tf.math.less(ious, 0.5), tf.float64), axis=1))\n",
    "        neg_order = tf.argsort(ious * tf.cast(tf.math.less(ious, 0.5), tf.float64), direction='DESCENDING', axis=1)[:, :neg_cnt]\n",
    "        indices = tf.range(start=0, limit=neg_cnt, dtype=tf.int32)\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "        neg_order = tf.gather(neg_order, shuffled_indices, axis=1)[:, :num_neg]\n",
    "\n",
    "    cls_labels = tf.concat([tf.ones_like(pos_order), tf.zeros_like(neg_order)], axis=1)\n",
    "    label_order = tf.concat([pos_order, neg_order], axis=1)\n",
    "    P_boxes = tf.gather(nms, label_order, batch_dims=1)\n",
    "    \n",
    "    tx = (gts[:, :1] - P_boxes[:, :, 0]) / P_boxes[:, :, 2]\n",
    "    ty = (gts[:, 1:2] - P_boxes[:, :, 1]) / P_boxes[:, :, 3]\n",
    "    tw = tf.math.log(gts[:, 2:3] / P_boxes[:, :, 2]) \n",
    "    th = tf.math.log(gts[:, 3:] / P_boxes[:, :, 3]) \n",
    "    tf.stack([tx, ty, tw, th], axis=-1)\n",
    "    box_labels = tf.stack([tx, ty, tw, th], axis=-1)\n",
    "    return box_labels, cls_labels, P_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-commission",
   "metadata": {},
   "source": [
    "classifier에서 label은 각 object별로 confidence score와 bounding box regression을 수행하는데 주어진 데이터와 task는 단 한명의 사람만 검출하면 되므로 rpn과 비슷한 과정을 거칩니다. \n",
    "\n",
    "다른점은 classifer에서는 추출된 지역과 ground truth의 iou가 0.5이상인 값이 P이고 나머지가 N이 됩니다.\n",
    "\n",
    "여기서 각 이미지당 128개의 label을 설정하는데 논문에서는 25% P를 하였지만 저는 50% 로 하였습니다. \n",
    "이는 단! 한명의 사람만 검출하는 과정 + data상에서 특정 위치에 사람이 있는경우가 많아 classifier가 조금더 다양한 위치를 포착하게 하기 위해 조치 하였습니다.\n",
    "\n",
    "마찬가지로 bounding box regression과정도 거칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-assessment",
   "metadata": {},
   "source": [
    "$$ \\int_{a}^{b}f(x) dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-developer",
   "metadata": {},
   "source": [
    "$$a \\sim b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-trash",
   "metadata": {},
   "source": [
    "$$a \\sum_{i=1}^{N} b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(tf.keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Classifier, self).__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(2048, kernel_size=(7, 7), name='conv')\n",
    "        self.bn = tf.keras.layers.BatchNormalization(name='conv_bn')\n",
    "        self.relu = tf.keras.layers.ReLU(name='conv_relu')\n",
    "        self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, name='dense1')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(name='dense1_bn')\n",
    "        self.relu1 = tf.keras.layers.ReLU(name='dense1_relu')\n",
    "\n",
    "        self.dense2 = tf.keras.layers.Dense(1024, name='dense2')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(name='dense2_bn')\n",
    "        self.relu2 = tf.keras.layers.ReLU(name='dense2_relu')\n",
    "\n",
    "        self.cls_dense = tf.keras.layers.Dense(256, name='cls_dense')\n",
    "        self.cls_bn = tf.keras.layers.BatchNormalization(, name='cls_bn')\n",
    "        self.cls_relu = tf.keras.layers.ReLU(name='cls_relu')\n",
    "        self.cls = tf.keras.layers.Dense(1, activation='sigmoid', name='cls_out')\n",
    "\n",
    "        self.bbox_dense = tf.keras.layers.Dense(256, name='bbox_dense')\n",
    "        self.bbox_bn = tf.keras.layers.BatchNormalization(name='bbox_bn')\n",
    "        self.bbox_relu = tf.keras.layers.ReLU(name='bbox_relu')\n",
    "        self.bbox_reg = tf.keras.layers.Dense(4, activation='relu', name='bbox_out')\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super(Classifier, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.test_loss_tracker = tf.keras.metrics.Mean(name='test_loss')\n",
    "    \n",
    "    def Cls_Loss(self, y_true, y_pred):\n",
    "        return tf.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "\n",
    "    def Reg_Loss(self, y_true, y_pred, indices):\n",
    "        return tf.losses.Huber()(y_true[indices], y_pred[indices])\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        indices = tf.not_equal(y_cls, 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            cls, bbox_reg, _ = self(x, training=True)\n",
    "            cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "            reg_loss = self.Reg_Loss(y_reg, bbox_reg, indices)\n",
    "            losses = cls_loss + reg_loss \n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        grad = tape.gradient(losses, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grad, trainable_vars))\n",
    "        self.loss_tracker.update_state(losses)\n",
    "        return {'classifier_loss': self.loss_tracker.result()}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        indices = tf.not_equal(y_cls, 0)\n",
    "\n",
    "        cls, bbox_reg, _ = self(x, training=False)\n",
    "        cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "        reg_loss = self.Reg_Loss(y_reg, bbox_reg, indices)\n",
    "        losses = cls_loss + reg_loss \n",
    "\n",
    "        self.test_loss_tracker.update_state(losses)\n",
    "        return {'classifier_loss_val': self.test_loss_tracker.result()}\n",
    "\n",
    "    \n",
    "    def bbox_regression(self, bbox, nmses):\n",
    "        tx = (bbox[:, :, 0] - nmses[:, :, 0]) / nmses[:, :, 2]\n",
    "        ty = (bbox[:, :, 1] - nmses[:, :, 1]) / nmses[:, :, 3]\n",
    "        tw = tf.math.log(tf.maximum(bbox[:, :, 2], np.finfo(np.float64).eps) / nmses[:, :, 2])\n",
    "        th = tf.math.log(tf.maximum(bbox[:, :, 3], np.finfo(np.float64).eps) / nmses[:, :, 3])\n",
    "        return tf.stack([tx, ty, tw, th], -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse_bbox_regression(bbox, nmses):\n",
    "        gx = nmses[:, :, 2] * bbox[:, :, 0] + nmses[:, :, 0]\n",
    "        gy = nmses[:, :, 3] * bbox[:, :, 1] + nmses[:, :, 1]\n",
    "        gw = nmses[:, :, 2] * tf.exp(bbox[:, :, 2])\n",
    "        gh = nmses[:, :, 3] * tf.exp(bbox[:, :, 3])\n",
    "        return tf.stack([gx, gy, gw, gh], -1)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        rois, nms = inputs\n",
    "\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.conv)(rois)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.bn)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.relu)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.flatten)(cls_x)\n",
    "\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.dense1)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.bn1)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.relu1)(cls_x)\n",
    "        \n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.dense2)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.bn2)(cls_x)\n",
    "        feature_vector = tf.keras.layers.TimeDistributed(self.relu2)(cls_x)\n",
    "\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.cls_dense)(feature_vector)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.cls_bn)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.cls_relu)(cls_x)\n",
    "\n",
    "        bbox_x = tf.keras.layers.TimeDistributed(self.bbox_dense)(feature_vector)\n",
    "        bbox_x = tf.keras.layers.TimeDistributed(self.bbox_bn)(bbox_x)\n",
    "        bbox_x = tf.keras.layers.TimeDistributed(self.bbox_relu)(bbox_x)\n",
    "\n",
    "        clss = tf.keras.layers.TimeDistributed(self.cls)(cls_x)\n",
    "        bbox = tf.keras.layers.TimeDistributed(self.bbox_reg)(bbox_x)\n",
    "        bbox_reg = self.bbox_regression(bbox, nms)\n",
    "\n",
    "        return clss, bbox_reg, nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-terminology",
   "metadata": {},
   "source": [
    "Classifier는 각 이미지당 128개의 roi가 존재 하기 때문에 TimeDistributed를 통하여 구현하였습니다.\n",
    "전체 적인 구조는 논문을 따르고 하이퍼파라미터 탐색과 batch norm을 추가하여 훈련에 안정성을 주었습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faster_RCNN(tf.keras.models.Model):\n",
    "    def __init__(self, img_size, anchor_boxes, k, n_sample, backbone, rpn_lambda, pool_size, **kwargs):\n",
    "        super(Faster_RCNN, self).__init__(*kwargs)\n",
    "        self.img_size = img_size\n",
    "        self.anchor_boxes = anchor_boxes\n",
    "        self.k = k\n",
    "        self.n_sample = n_sample\n",
    "        self.backbone = backbone\n",
    "        self.rpn_lambda = rpn_lambda\n",
    "        self.pool_size = pool_size\n",
    "        self.n_train_pre_nms = 6000\n",
    "        self.n_train_post_nms = 1000\n",
    "        self.n_test_pre_nms = 3000\n",
    "        self.n_test_post_nms = 128\n",
    "        self.iou_threshold = 0.7\n",
    "\n",
    "        self.rpn = RPN(\n",
    "            img_size= self.img_size, \n",
    "            anchor_boxes=self.anchor_boxes, \n",
    "            k=self.k, \n",
    "            n_sample=self.n_sample, \n",
    "            backbone=self.backbone, \n",
    "            rpn_lambda=self.rpn_lambda,\n",
    "            name='rpn'\n",
    "            )\n",
    "        self.get_candidate = get_candidate_layer(name='get_candidate')\n",
    "        self.get_nms = NMS(iou_threshold=self.iou_threshold, name='get_nms')\n",
    "        self.roipool = RoIpool(pool_size=self.pool_size, name='roipool')\n",
    "        self.classifier = Classifier(name='classifier')\n",
    "\n",
    "    def compile(self, rpn_optimizer, classifier_optimizer):\n",
    "        self.rpn.compile(optimizer=rpn_optimizer)\n",
    "        self.classifier.compile(optimizer=classifier_optimizer)\n",
    "        super(Faster_RCNN, self).compile()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        scores, rps, feature_map = self.rpn(inputs)\n",
    "        rps = self.rpn.inverse_bbox_regression(rps)\n",
    "        candidate_area, scores = self.get_candidate((scores, rps, self.n_test_pre_nms))\n",
    "        nms = self.get_nms((candidate_area, scores, self.n_test_post_nms))\n",
    "        rois = self.roipool((feature_map, nms))\n",
    "        cls, bbox_reg, nms = self.classifier((rois, nms))\n",
    "        predict = self.classifier.inverse_bbox_regression(bbox_reg, nms)\n",
    "        return cls, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-first",
   "metadata": {},
   "source": [
    "rpn, roipooling, nms, classifier를 합친 Faster_RCNN을 구현하였습니다.\n",
    "논문에서 end-to-end라고 하였는데 training stage가 4단계로 나누어져 있어 train_step을 따로 구현하여 훈련되도록 하였습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcnn_train_step(model, train_dataset, train_stage, epochs=1, valid_dataset=None, change_lr=False, rpn_lr=None, cls_lr=None):\n",
    "    if change_lr:\n",
    "        if rpn_lr:\n",
    "            tf.keras.backend.set_value(model.rpn.optimizer.learning_rate, rpn_lr)\n",
    "        if cls_lr:\n",
    "            tf.keras.backend.set_value(model.classifier.optimizer.learning_rate, cls_lr)\n",
    "\n",
    "    if train_stage == 1:\n",
    "        print('Train RPNs \\n')\n",
    "        model.rpn.trainable = True\n",
    "        model.classifier.trainable = False\n",
    "    elif train_stage == 2:\n",
    "        print('Train Fast R-CNN using the proposals from RPNs \\n')\n",
    "        model.rpn.trainable = False\n",
    "        model.rpn.base_model.trainable = True\n",
    "        model.classifier.trainable = True\n",
    "    elif train_stage == 3:\n",
    "        print('Fix the shared convolutional layers and fine-tune unique layers to RPN \\n')\n",
    "        model.rpn.trainable = True\n",
    "        model.rpn.base_model.trainable = False\n",
    "        model.classifier.trainable = False\n",
    "    elif train_stage == 4:\n",
    "        print('Fine-tune unique layers to Fast R-CNN \\n')\n",
    "        model.rpn.trainable = False\n",
    "        model.classifier.trainable = True\n",
    "\n",
    "    max_step = 'Unknown'\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"epoch {epoch+1}/{epochs}\")\n",
    "        display_loss = display(\"Training loss (for one batch) at step 0 : 0\", display_id=True)\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            start = time.time()\n",
    "            y_cls_rpn, y_reg_rpn, gts = y_batch_train\n",
    "            \n",
    "            if train_stage == 1 or train_stage == 3:\n",
    "                result = model.rpn.train_step((x_batch_train, (y_cls_rpn, y_reg_rpn)))\n",
    "                losses = round(float(result['rpn_loss'].numpy()), 5)\n",
    "            else:\n",
    "                scores, rps, feature_map = model.rpn(x_batch_train, training=False)\n",
    "                if train_stage == 2:\n",
    "                    model.rpn.train_step((x_batch_train, (y_cls_rpn, y_reg_rpn)))\n",
    "                rps = model.rpn.inverse_bbox_regression(rps)\n",
    "                candidate_area, scores = model.get_candidate((scores, rps, model.n_train_pre_nms))\n",
    "                nms = model.get_nms((candidate_area, scores, model.n_train_post_nms))\n",
    "                box_labels, cls_labels, nms = classifier_label_generator(nms, gts)\n",
    "                rois = model.roipool((feature_map, nms))\n",
    "                result = model.classifier.train_step(((rois, nms), (cls_labels, box_labels)))\n",
    "                losses = round(float(result['classifier_loss'].numpy()), 5)\n",
    "\n",
    "            display_loss.update(f\"Training loss at step {step}/{max_step} : {losses} - {round(time.time() - start, 4)}sec/step - {time.strftime('%Hh%Mm%Ss', time.gmtime(time.time()-epoch_start))}/epoch\")\n",
    "        max_step = step\n",
    "        display_loss.update(f\"Training loss at step {step}/{max_step} : {losses} - {round(time.time()-start, 4)}sec/step - {time.strftime('%Hh%Mm%Ss', time.gmtime(time.time()-epoch_start))}/epoch\")\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            display_loss_valid = display(\"validation loss : 0\", display_id=True)\n",
    "            for x_batch_test, y_batch_test in valid_dataset:\n",
    "                y_cls_rpn, y_reg_rpn, gts = y_batch_test\n",
    "\n",
    "                if train_stage == 1 or train_stage == 3:\n",
    "                    result = model.rpn.test_step((x_batch_test, (y_cls_rpn, y_reg_rpn)))\n",
    "                    losses = round(float(result['rpn_loss_val'].numpy()), 5)\n",
    "                else:\n",
    "                    scores, rps, feature_map = model.rpn.predict(x_batch_test)\n",
    "                    rps = model.rpn.inverse_bbox_regression(rps)\n",
    "                    candidate_area, scores = model.get_candidate((scores, rps, model.n_test_pre_nms))\n",
    "                    nms = model.get_nms((candidate_area, scores, model.n_test_post_nms))\n",
    "                    box_labels, cls_labels, nms = classifier_label_generator(nms, gts, valid=True)\n",
    "                    rois = model.roipool((feature_map, nms))\n",
    "                    result = model.classifier.test_step(((rois, nms), (cls_labels, box_labels)))\n",
    "                    losses = round(float(result['classifier_loss_val'].numpy()), 5)\n",
    "                \n",
    "            display_loss_valid.update(f\"validation loss : {losses}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-refrigerator",
   "metadata": {},
   "source": [
    "훈련 데이터가 3760개 에다가 연속적인 장면이므로 augmentation을 거쳐도 다양하지 않습니다. 또한 test data의 경우는 철봉과 같이 train data에 존재하지 않는 기구들로 인해 사람이 가려지기 때문에 오버피팅을 특히 조심하여 훈련 시키도록 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stage = 1\n",
    "epochs = 10\n",
    "\n",
    "frcnn = frcnn_train_step(\n",
    "    model=frcnn, \n",
    "    train_dataset=train_dataset, \n",
    "    valid_dataset=valid_dataset,\n",
    "    train_stage=train_stage,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-apple",
   "metadata": {},
   "source": [
    "아래는 각 훈련 stage가 끝난뒤 결과를 시각화 한것입니다. \n",
    "빨간색 박스가 ground truth이며 이외에 box가 detection 결과 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-hospital",
   "metadata": {},
   "source": [
    "label setting 부터 모델 구조, 하이퍼 파라미터까지 고민하면서 좋은 경험이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-corrections",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
